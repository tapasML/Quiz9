{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MNIST_Session_4_Arch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tapasML/Quiz9/blob/main/MNIST_Session_4_Arch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzIt-tFil7KZ",
        "outputId": "44d86a57-d36b-4d8b-d42d-93109df5ed01"
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "now = datetime.now()\r\n",
        "\r\n",
        "current_time = now.strftime(\"%H:%M:%S\")\r\n",
        "print(\"Current Time =\", current_time)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Time = 23:06:35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGpmrS1peESC"
      },
      "source": [
        "**Install required packages**\r\n",
        "\r\n",
        "select GPU as device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246062e0-7176-4f0c-c606-980e6f9232e2"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVRAYfJpemG4"
      },
      "source": [
        "**Define the Network**\r\n",
        "\r\n",
        "Since parameters size is retricted, we can not suddenly expand and reduce channels in a layer, as it hurts learning weights. \r\n",
        "Instead start small (10 channels) and increase uniformly in baby steps.\r\n",
        "\r\n",
        "Using padding in first two blocks, to preserve every pixel of information we got.\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Block #1:\r\n",
        "\r\n",
        "[Conv-> ReLU-> BatchNorm] -> [Conv-> ReLU-> BatchNorm] -> MaxPool -> Dropout\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "Block #2:\r\n",
        "\r\n",
        "[Conv-> ReLU-> BatchNorm] -> [Conv-> ReLU-> BatchNorm] -> MaxPool -> Dropout\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "Block #3:\r\n",
        "\r\n",
        "[Conv-> GAP]\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "Block #4:\r\n",
        "\r\n",
        "[Flatten -> Log_SoftMax]\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3,  32, 3, padding=1) \n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1) \n",
        "        self.pool1 = nn.MaxPool2d(2, 2)      #16        \n",
        "        \n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1) \n",
        "        self.conv5 = nn.Conv2d(64, 64, 3, padding=1) \n",
        "        self.pool2 = nn.MaxPool2d(2, 2)   #8\n",
        "        \n",
        "            \n",
        "        self.conv6 = nn.Conv2d(64, 128, 3, padding=0) \n",
        "        self.conv7 = nn.Conv2d(128, 128, 3, padding=0)  \n",
        "        self.conv8 = nn.Conv2d(128, 128, 3, padding=1)                    \n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=4, stride=4)\n",
        "\n",
        "        self.batchNorm_1 = nn.BatchNorm2d(32)\n",
        "        self.batchNorm_2 = nn.BatchNorm2d(32)\n",
        "        self.batchNorm_3 = nn.BatchNorm2d(64)\n",
        "        self.batchNorm_4 = nn.BatchNorm2d(64)\n",
        "        self.batchNorm_5 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.batchNorm_6 = nn.BatchNorm2d(128)\n",
        "        self.batchNorm_7 = nn.BatchNorm2d(128)\n",
        "        self.batchNorm_8 = nn.BatchNorm2d(128)\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "\n",
        "      \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.batchNorm_2(F.relu(self.conv2(self.batchNorm_1(F.relu(self.conv1(x)))))))\n",
        "        \n",
        "        y= self.batchNorm_4(F.relu(self.conv4(  self.batchNorm_3(F.relu(self.conv3(x))))))\n",
        "        x= self.batchNorm_5(F.relu(self.conv5(y)))\n",
        "        x=  self.pool1(x)\n",
        "        z = self.batchNorm_7(F.relu(self.conv7(  self.batchNorm_6(F.relu(self.conv6(x))))))\n",
        "        x = self.batchNorm_8(F.relu(self.conv8(z)))\n",
        "                      \n",
        "        x = self.avg_pool(x) \n",
        "        x = x.view(-1, 128)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmybhdUBjnX6"
      },
      "source": [
        "**Instatiate Network**\r\n",
        "\r\n",
        "Assign GPU to network model\r\n",
        "\r\n",
        "Print the model\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdydjYTZFyi3",
        "outputId": "6ee18125-6247-4778-a01c-86f69aeba372"
      },
      "source": [
        "model = Net().to(device)\n",
        "summary(model, input_size=(3,32,32))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "            Conv2d-3           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
            "         MaxPool2d-5           [-1, 32, 16, 16]               0\n",
            "            Conv2d-6           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-7           [-1, 64, 16, 16]             128\n",
            "            Conv2d-8           [-1, 64, 16, 16]          36,928\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "           Conv2d-10           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-11           [-1, 64, 16, 16]             128\n",
            "        MaxPool2d-12             [-1, 64, 8, 8]               0\n",
            "           Conv2d-13            [-1, 128, 6, 6]          73,856\n",
            "      BatchNorm2d-14            [-1, 128, 6, 6]             256\n",
            "           Conv2d-15            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-16            [-1, 128, 4, 4]             256\n",
            "           Conv2d-17            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-18            [-1, 128, 4, 4]             256\n",
            "        AvgPool2d-19            [-1, 128, 1, 1]               0\n",
            "           Linear-20                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 474,090\n",
            "Trainable params: 474,090\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.98\n",
            "Params size (MB): 1.81\n",
            "Estimated Total Size (MB): 3.80\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU097_L6kQKV"
      },
      "source": [
        "**Load Train and Test data**\r\n",
        "\r\n",
        "set Block size.\r\n",
        "\r\n",
        "Since we are using BatchNormalization, we should not normalize the data data while loading\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa09182-a977-4c9e-ebef-7f248b833de0"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR10('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor()\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFV0is8Gkpou"
      },
      "source": [
        "**Train and Test Network Flow**\r\n",
        "\r\n",
        "Print Logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "        tqdm._instances.clear()       \n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    tqdm._instances.clear()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztq3YQ9Ok4L4"
      },
      "source": [
        "**Execute The Network**\r\n",
        "\r\n",
        "Using SGD with learning rate = 0.01 and momentum.\r\n",
        "\r\n",
        "Since majorority of learning is done by 10 epochs, after 10, reduce the learning rate (using a scheduler) so as to reduce overshooting of weights\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMWbLWO6FuHb",
        "outputId": "a4241bdf-5cec-445f-d52d-674023a65de5"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "for epoch in range(1, 25):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=0.9001146554946899 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 31.94it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.9189, Accuracy: 6733/10000 (67.33%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.47179263830184937 batch_id=390: 100%|██████████| 391/391 [00:11<00:00, 33.41it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.7239, Accuracy: 7485/10000 (74.85%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.6154791116714478 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 31.90it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.6158, Accuracy: 7894/10000 (78.94%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.6148496866226196 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 32.23it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.6073, Accuracy: 7895/10000 (78.95%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.3904905617237091 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 31.70it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5440, Accuracy: 8186/10000 (81.86%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.35304635763168335 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 32.45it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5822, Accuracy: 8071/10000 (80.71%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.29712095856666565 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 32.24it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5474, Accuracy: 8216/10000 (82.16%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.34353044629096985 batch_id=390: 100%|██████████| 391/391 [00:11<00:00, 32.61it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5473, Accuracy: 8311/10000 (83.11%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.34219232201576233 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 32.36it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5693, Accuracy: 8338/10000 (83.38%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.23331637680530548 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 31.33it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5838, Accuracy: 8308/10000 (83.08%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.08692455291748047 batch_id=390: 100%|██████████| 391/391 [00:11<00:00, 32.62it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.4924, Accuracy: 8539/10000 (85.39%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.042441558092832565 batch_id=390: 100%|██████████| 391/391 [00:12<00:00, 31.16it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.4982, Accuracy: 8527/10000 (85.27%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.03397040441632271 batch_id=335:  86%|████████▌ | 335/391 [00:10<00:01, 30.92it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}